{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8375930,"sourceType":"datasetVersion","datasetId":4822755},{"sourceId":8904934,"sourceType":"datasetVersion","datasetId":5353854},{"sourceId":8905257,"sourceType":"datasetVersion","datasetId":5354105}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine tuning a BERT model using a labeled Amazon Electronics review dataset","metadata":{}},{"cell_type":"markdown","source":"In this fine tuning, I used an already labeled dataset in Kaggle. https://www.kaggle.com/datasets/jimtsiobikas/amazon-reviews-2018-electronics \nThis dataset contains the reviewText and all these texts are classified as POSITIVE, NEGATIVE or NEUTRAL.\nThis dataset have two csv files and here I use the 60k csv file.\nDue to computational power constraints, here I chose a random sample of 1.5k data samples with 0.5k reviews from each target class.","metadata":{}},{"cell_type":"markdown","source":"# Hyper parameter tuning","metadata":{}},{"cell_type":"markdown","source":"### Preparing the dataset to fine tune ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/amazon-reviews-2018-electronics/labeled_electronics_dataset_60k.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-09T06:47:18.460006Z","iopub.execute_input":"2024-07-09T06:47:18.460400Z","iopub.status.idle":"2024-07-09T06:47:18.875930Z","shell.execute_reply.started":"2024-07-09T06:47:18.460372Z","shell.execute_reply":"2024-07-09T06:47:18.874964Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   overall  vote  verified  reviewTime  \\\n0        5     0      True  2013-11-14   \n1        5     0      True  2013-04-23   \n2        4     0      True  2017-05-31   \n3        1     0      True  2014-11-25   \n4        5     0      True  2015-02-21   \n\n                                          reviewText  \\\n0  Have had this approx 6 months now - is much fa...   \n1  I bought my first camera bag with my camera (a...   \n2  I use this case for a Panasonic Lumix ZS50 cam...   \n3           Made no difference to my Sony camcorder.   \n4                      Raspbmc users will love this!   \n\n                                             summary     Label  \n0                                     Great NAS Unit  POSITIVE  \n1                                          Great Bag  POSITIVE  \n2  Decent Case for a Good Price -- With Some Caveats  POSITIVE  \n3                                           One Star  NEGATIVE  \n4                                         1424476800  POSITIVE  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>overall</th>\n      <th>vote</th>\n      <th>verified</th>\n      <th>reviewTime</th>\n      <th>reviewText</th>\n      <th>summary</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>0</td>\n      <td>True</td>\n      <td>2013-11-14</td>\n      <td>Have had this approx 6 months now - is much fa...</td>\n      <td>Great NAS Unit</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>0</td>\n      <td>True</td>\n      <td>2013-04-23</td>\n      <td>I bought my first camera bag with my camera (a...</td>\n      <td>Great Bag</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>0</td>\n      <td>True</td>\n      <td>2017-05-31</td>\n      <td>I use this case for a Panasonic Lumix ZS50 cam...</td>\n      <td>Decent Case for a Good Price -- With Some Caveats</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>True</td>\n      <td>2014-11-25</td>\n      <td>Made no difference to my Sony camcorder.</td>\n      <td>One Star</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>True</td>\n      <td>2015-02-21</td>\n      <td>Raspbmc users will love this!</td>\n      <td>1424476800</td>\n      <td>POSITIVE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"cols = ['reviewText','Label']\ndf = df[cols]\ndf = df.rename(columns = {'reviewText':'text'})\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-09T06:47:22.105865Z","iopub.execute_input":"2024-07-09T06:47:22.106540Z","iopub.status.idle":"2024-07-09T06:47:22.129014Z","shell.execute_reply.started":"2024-07-09T06:47:22.106506Z","shell.execute_reply":"2024-07-09T06:47:22.128110Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                                text     Label\n0  Have had this approx 6 months now - is much fa...  POSITIVE\n1  I bought my first camera bag with my camera (a...  POSITIVE\n2  I use this case for a Panasonic Lumix ZS50 cam...  POSITIVE\n3           Made no difference to my Sony camcorder.  NEGATIVE\n4                      Raspbmc users will love this!  POSITIVE","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Have had this approx 6 months now - is much fa...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I bought my first camera bag with my camera (a...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I use this case for a Panasonic Lumix ZS50 cam...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Made no difference to my Sony camcorder.</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Raspbmc users will love this!</td>\n      <td>POSITIVE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Using only 1500 records from the dataset, containing 500 reviews from each sentiment\ndf_selected = df.groupby('Label', group_keys=False).apply(lambda x: x.sample(min(len(x), 500)))\n\n# Reset index to clean up the dataframe\ndf_selected = df_selected.reset_index(drop=True)\n\n# Converting NEGATIVE, NEUTRAL and POSITIVE labels to 0, 1 and 2 respectively.\ndf_selected['Label'] = df_selected['Label'].map({'NEGATIVE': 0, 'NEUTRAL': 1, 'POSITIVE': 2})\n\ndf_selected","metadata":{"execution":{"iopub.status.busy":"2024-07-09T06:47:24.629939Z","iopub.execute_input":"2024-07-09T06:47:24.630307Z","iopub.status.idle":"2024-07-09T06:47:24.667046Z","shell.execute_reply.started":"2024-07-09T06:47:24.630278Z","shell.execute_reply":"2024-07-09T06:47:24.666114Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3940072638.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_selected = df.groupby('Label', group_keys=False).apply(lambda x: x.sample(min(len(x), 500)))\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                                   text  Label\n0                                     Does not deliver.      0\n1     This unit would not work with chromecast.  It ...      0\n2     The problem is not precisely the discs themsel...      0\n3     After hearing and listening to reviews and vid...      0\n4                                Failed after 10 months      0\n...                                                 ...    ...\n1495               Good Simple Speakers At A Good Price      2\n1496                              Small and Easy to Use      2\n1497                                       Fits perfect      2\n1498  Very nice range - 24-100mm. Like the clutch fo...      2\n1499  It works great, hello to Dick Tracy for those ...      2\n\n[1500 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Does not deliver.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This unit would not work with chromecast.  It ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The problem is not precisely the discs themsel...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>After hearing and listening to reviews and vid...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Failed after 10 months</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1495</th>\n      <td>Good Simple Speakers At A Good Price</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1496</th>\n      <td>Small and Easy to Use</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1497</th>\n      <td>Fits perfect</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1498</th>\n      <td>Very nice range - 24-100mm. Like the clutch fo...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1499</th>\n      <td>It works great, hello to Dick Tracy for those ...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>1500 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Saving the prepared dataset","metadata":{}},{"cell_type":"code","source":"df_selected.to_csv('/kaggle/working/selected.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-09T06:47:28.818234Z","iopub.execute_input":"2024-07-09T06:47:28.818595Z","iopub.status.idle":"2024-07-09T06:47:28.843994Z","shell.execute_reply.started":"2024-07-09T06:47:28.818569Z","shell.execute_reply":"2024-07-09T06:47:28.843276Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Loading the model, data and tuning hyper-parameters","metadata":{}},{"cell_type":"code","source":"import optuna\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import load_dataset\n\ndef objective(trial):\n    # Define the hyperparameters to tune\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True)\n    batch_size = trial.suggest_categorical('batch_size', [4, 8, 16])\n    num_train_epochs = trial.suggest_int('num_train_epochs', 2, 5)  # Reduced max epochs\n    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n    gradient_accumulation_steps = trial.suggest_int('gradient_accumulation_steps', 1, 4)\n\n    model_name = 'bert-base-uncased'\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    dataset_path = '/kaggle/working/selected.csv'\n    # Load the CSV file as a dataset\n    dataset = load_dataset('csv', data_files=dataset_path)\n    \n    # Since the dataset is loaded with a single key, we access it with 'train'\n    dataset = dataset['train']\n\n    # Split the dataset into train and test sets\n    split_dataset = dataset.train_test_split(test_size=0.2)\n    \n    # Access the splits\n    train_dataset = split_dataset['train']\n    eval_dataset = split_dataset['test']\n\n    def preprocess_function(examples):\n        inputs = tokenizer(examples['text'], truncation=True, padding='max_length')\n        inputs['labels'] = examples['Label']\n        return inputs\n    \n    train_dataset = train_dataset.map(preprocess_function, batched=True)\n    eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n    \n    # Set format for PyTorch\n    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    \n    args = TrainingArguments(\n        output_dir='./results',\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        learning_rate=learning_rate,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=num_train_epochs,\n        weight_decay=weight_decay,\n        save_total_limit=1,\n        metric_for_best_model='accuracy',\n        load_best_model_at_end=True,\n        fp16=True,  # Enable mixed precision training\n        gradient_accumulation_steps=gradient_accumulation_steps,\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=compute_metrics\n    )\n    \n    trainer.train()\n    eval_result = trainer.evaluate()\n    return eval_result['eval_accuracy']\n\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    labels = p.label_ids\n    accuracy = accuracy_score(labels, preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-09T04:10:41.609767Z","iopub.execute_input":"2024-07-09T04:10:41.610633Z","iopub.status.idle":"2024-07-09T05:45:40.167983Z","shell.execute_reply.started":"2024-07-09T04:10:41.610602Z","shell.execute_reply":"2024-07-09T05:45:40.167206Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"[I 2024-07-09 04:10:41,622] A new study created in memory with name: no-name-d5044e11-0bfa-4954-9ab7-7040de9b14f0\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbd84069e4434ee8a188165b23b095fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39f2448cf7ae4cacad76dce48740d8b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79c2338f5ba14e5a86a77ff9f02654c9"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240709_041153-drkpd53o</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/chethmi-21-University%20of%20Moratuwa/huggingface/runs/drkpd53o' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/chethmi-21-University%20of%20Moratuwa/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/chethmi-21-University%20of%20Moratuwa/huggingface' target=\"_blank\">https://wandb.ai/chethmi-21-University%20of%20Moratuwa/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/chethmi-21-University%20of%20Moratuwa/huggingface/runs/drkpd53o' target=\"_blank\">https://wandb.ai/chethmi-21-University%20of%20Moratuwa/huggingface/runs/drkpd53o</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [72/72 04:22, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>1.069563</td>\n      <td>0.430000</td>\n      <td>0.516911</td>\n      <td>0.470130</td>\n      <td>0.414059</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.026562</td>\n      <td>0.483333</td>\n      <td>0.540908</td>\n      <td>0.479186</td>\n      <td>0.467584</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.965326</td>\n      <td>0.606667</td>\n      <td>0.610569</td>\n      <td>0.608747</td>\n      <td>0.609287</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.948658</td>\n      <td>0.620000</td>\n      <td>0.631711</td>\n      <td>0.618774</td>\n      <td>0.623230</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19/19 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 04:16:43,531] Trial 0 finished with value: 0.62 and parameters: {'learning_rate': 1.1479499103671342e-05, 'batch_size': 8, 'num_train_epochs': 4, 'weight_decay': 3.536692045001775e-05, 'gradient_accumulation_steps': 4}. Best is trial 0 with value: 0.62.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b8c3ff4bb6a48459d1f3bb5246b80c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"579a3ea8f0b84437b14baab7678bab71"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [225/225 04:04, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.759314</td>\n      <td>0.670000</td>\n      <td>0.699338</td>\n      <td>0.669546</td>\n      <td>0.675384</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.737977</td>\n      <td>0.713333</td>\n      <td>0.720848</td>\n      <td>0.709718</td>\n      <td>0.712101</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.728723</td>\n      <td>0.700000</td>\n      <td>0.711055</td>\n      <td>0.699009</td>\n      <td>0.702129</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 04:20:57,578] Trial 1 finished with value: 0.7133333333333334 and parameters: {'learning_rate': 3.656913249721955e-05, 'batch_size': 4, 'num_train_epochs': 3, 'weight_decay': 0.0009587435553273778, 'gradient_accumulation_steps': 2}. Best is trial 1 with value: 0.7133333333333334.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d4b26c21394afc9cc69b4d92b46470"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [36/36 02:11, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>1.059446</td>\n      <td>0.490000</td>\n      <td>0.488125</td>\n      <td>0.485704</td>\n      <td>0.483944</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.030336</td>\n      <td>0.500000</td>\n      <td>0.497608</td>\n      <td>0.497479</td>\n      <td>0.496752</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19/19 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 04:23:18,799] Trial 2 finished with value: 0.5 and parameters: {'learning_rate': 1.1491064712924295e-05, 'batch_size': 8, 'num_train_epochs': 2, 'weight_decay': 6.747909306857522e-06, 'gradient_accumulation_steps': 4}. Best is trial 1 with value: 0.7133333333333334.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [300/300 02:48, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.723392</td>\n      <td>0.670000</td>\n      <td>0.674405</td>\n      <td>0.666460</td>\n      <td>0.668984</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.770960</td>\n      <td>0.700000</td>\n      <td>0.706121</td>\n      <td>0.697020</td>\n      <td>0.699806</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 04:26:15,491] Trial 3 finished with value: 0.7 and parameters: {'learning_rate': 4.253132899789529e-05, 'batch_size': 4, 'num_train_epochs': 2, 'weight_decay': 0.0015810486175104057, 'gradient_accumulation_steps': 1}. Best is trial 1 with value: 0.7133333333333334.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [36/36 02:11, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>1.041870</td>\n      <td>0.526667</td>\n      <td>0.529206</td>\n      <td>0.522242</td>\n      <td>0.521680</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.014362</td>\n      <td>0.560000</td>\n      <td>0.573123</td>\n      <td>0.556834</td>\n      <td>0.558175</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19/19 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 04:28:36,540] Trial 4 finished with value: 0.56 and parameters: {'learning_rate': 1.3302023688773855e-05, 'batch_size': 8, 'num_train_epochs': 2, 'weight_decay': 0.0008195787205256619, 'gradient_accumulation_steps': 4}. Best is trial 1 with value: 0.7133333333333334.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [57/57 03:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.043436</td>\n      <td>0.536667</td>\n      <td>0.540304</td>\n      <td>0.530646</td>\n      <td>0.526400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.004950</td>\n      <td>0.566667</td>\n      <td>0.563739</td>\n      <td>0.560594</td>\n      <td>0.557567</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.989148</td>\n      <td>0.560000</td>\n      <td>0.544124</td>\n      <td>0.552399</td>\n      <td>0.543174</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:04]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 04:32:04,034] Trial 5 finished with value: 0.5666666666666667 and parameters: {'learning_rate': 1.0615549502952573e-05, 'batch_size': 16, 'num_train_epochs': 3, 'weight_decay': 1.3266096412488503e-05, 'gradient_accumulation_steps': 2}. Best is trial 1 with value: 0.7133333333333334.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 06:42, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.960336</td>\n      <td>0.536667</td>\n      <td>0.511076</td>\n      <td>0.528725</td>\n      <td>0.503747</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.848983</td>\n      <td>0.620000</td>\n      <td>0.602821</td>\n      <td>0.611914</td>\n      <td>0.601385</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.823920</td>\n      <td>0.630000</td>\n      <td>0.623028</td>\n      <td>0.625289</td>\n      <td>0.621048</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.795839</td>\n      <td>0.633333</td>\n      <td>0.613884</td>\n      <td>0.625241</td>\n      <td>0.613530</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.788727</td>\n      <td>0.663333</td>\n      <td>0.653645</td>\n      <td>0.658024</td>\n      <td>0.653870</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 04:38:55,462] Trial 6 finished with value: 0.6633333333333333 and parameters: {'learning_rate': 1.4100424038183036e-05, 'batch_size': 4, 'num_train_epochs': 5, 'weight_decay': 0.0010351300755642454, 'gradient_accumulation_steps': 3}. Best is trial 1 with value: 0.7133333333333334.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [375/375 05:56, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.930480</td>\n      <td>0.580000</td>\n      <td>0.647841</td>\n      <td>0.583923</td>\n      <td>0.585671</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.733714</td>\n      <td>0.683333</td>\n      <td>0.690366</td>\n      <td>0.680644</td>\n      <td>0.683341</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.750346</td>\n      <td>0.700000</td>\n      <td>0.708641</td>\n      <td>0.698763</td>\n      <td>0.700651</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.803367</td>\n      <td>0.706667</td>\n      <td>0.723298</td>\n      <td>0.705964</td>\n      <td>0.709528</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.815379</td>\n      <td>0.686667</td>\n      <td>0.688113</td>\n      <td>0.683579</td>\n      <td>0.685321</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19/19 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 04:44:59,580] Trial 7 finished with value: 0.7066666666666667 and parameters: {'learning_rate': 2.014799787126068e-05, 'batch_size': 8, 'num_train_epochs': 5, 'weight_decay': 2.254259309194505e-06, 'gradient_accumulation_steps': 1}. Best is trial 1 with value: 0.7133333333333334.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 04:36, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.052880</td>\n      <td>0.446667</td>\n      <td>0.528743</td>\n      <td>0.454042</td>\n      <td>0.427563</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.887336</td>\n      <td>0.603333</td>\n      <td>0.622826</td>\n      <td>0.603917</td>\n      <td>0.602584</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.843854</td>\n      <td>0.643333</td>\n      <td>0.661889</td>\n      <td>0.642909</td>\n      <td>0.646682</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.815726</td>\n      <td>0.663333</td>\n      <td>0.668651</td>\n      <td>0.660853</td>\n      <td>0.663437</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19/19 00:04]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 04:49:45,118] Trial 8 finished with value: 0.6633333333333333 and parameters: {'learning_rate': 2.139071590857557e-05, 'batch_size': 8, 'num_train_epochs': 4, 'weight_decay': 0.00011054333259594344, 'gradient_accumulation_steps': 3}. Best is trial 1 with value: 0.7133333333333334.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 02:13, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.841525</td>\n      <td>0.650000</td>\n      <td>0.644709</td>\n      <td>0.643571</td>\n      <td>0.640820</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.755032</td>\n      <td>0.696667</td>\n      <td>0.688947</td>\n      <td>0.691502</td>\n      <td>0.689418</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:04]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 04:52:06,998] Trial 9 finished with value: 0.6966666666666667 and parameters: {'learning_rate': 3.462697320271094e-05, 'batch_size': 16, 'num_train_epochs': 2, 'weight_decay': 9.130901551907551e-06, 'gradient_accumulation_steps': 1}. Best is trial 1 with value: 0.7133333333333334.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [225/225 04:01, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.765700</td>\n      <td>0.666667</td>\n      <td>0.691954</td>\n      <td>0.667497</td>\n      <td>0.671554</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.734898</td>\n      <td>0.696667</td>\n      <td>0.707663</td>\n      <td>0.693834</td>\n      <td>0.696546</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.765054</td>\n      <td>0.676667</td>\n      <td>0.678983</td>\n      <td>0.673809</td>\n      <td>0.675745</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 04:56:17,154] Trial 10 finished with value: 0.6966666666666667 and parameters: {'learning_rate': 3.0516483538413452e-05, 'batch_size': 4, 'num_train_epochs': 3, 'weight_decay': 0.006578914432965138, 'gradient_accumulation_steps': 2}. Best is trial 1 with value: 0.7133333333333334.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 06:58, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.797924</td>\n      <td>0.633333</td>\n      <td>0.646907</td>\n      <td>0.632042</td>\n      <td>0.635921</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.744568</td>\n      <td>0.696667</td>\n      <td>0.703520</td>\n      <td>0.695337</td>\n      <td>0.697493</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.766105</td>\n      <td>0.720000</td>\n      <td>0.734847</td>\n      <td>0.718165</td>\n      <td>0.721050</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.635200</td>\n      <td>0.881057</td>\n      <td>0.703333</td>\n      <td>0.701631</td>\n      <td>0.700326</td>\n      <td>0.700755</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.635200</td>\n      <td>0.980247</td>\n      <td>0.696667</td>\n      <td>0.699577</td>\n      <td>0.694966</td>\n      <td>0.696145</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 05:03:23,441] Trial 11 finished with value: 0.72 and parameters: {'learning_rate': 2.0826976349749076e-05, 'batch_size': 4, 'num_train_epochs': 5, 'weight_decay': 1.6916972103647168e-06, 'gradient_accumulation_steps': 1}. Best is trial 11 with value: 0.72.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [300/300 05:22, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.827679</td>\n      <td>0.673333</td>\n      <td>0.692230</td>\n      <td>0.673103</td>\n      <td>0.677178</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.744511</td>\n      <td>0.686667</td>\n      <td>0.693657</td>\n      <td>0.683567</td>\n      <td>0.685437</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.746561</td>\n      <td>0.706667</td>\n      <td>0.704177</td>\n      <td>0.703381</td>\n      <td>0.703552</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.779843</td>\n      <td>0.710000</td>\n      <td>0.712068</td>\n      <td>0.707664</td>\n      <td>0.709170</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 05:08:53,982] Trial 12 finished with value: 0.71 and parameters: {'learning_rate': 2.850022220026283e-05, 'batch_size': 4, 'num_train_epochs': 4, 'weight_decay': 0.00015822018094287406, 'gradient_accumulation_steps': 2}. Best is trial 11 with value: 0.72.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [450/450 04:11, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.758239</td>\n      <td>0.686667</td>\n      <td>0.696277</td>\n      <td>0.685065</td>\n      <td>0.688363</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.851120</td>\n      <td>0.680000</td>\n      <td>0.694356</td>\n      <td>0.675858</td>\n      <td>0.679457</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.943539</td>\n      <td>0.693333</td>\n      <td>0.721165</td>\n      <td>0.694009</td>\n      <td>0.698793</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 05:13:12,970] Trial 13 finished with value: 0.6933333333333334 and parameters: {'learning_rate': 4.591095160998311e-05, 'batch_size': 4, 'num_train_epochs': 3, 'weight_decay': 1.6746572752506658e-06, 'gradient_accumulation_steps': 1}. Best is trial 11 with value: 0.72.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [375/375 06:42, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.864820</td>\n      <td>0.593333</td>\n      <td>0.572281</td>\n      <td>0.584905</td>\n      <td>0.573074</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.776956</td>\n      <td>0.633333</td>\n      <td>0.635210</td>\n      <td>0.627698</td>\n      <td>0.628558</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.731661</td>\n      <td>0.683333</td>\n      <td>0.684177</td>\n      <td>0.679912</td>\n      <td>0.681587</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.732464</td>\n      <td>0.696667</td>\n      <td>0.692958</td>\n      <td>0.692365</td>\n      <td>0.692626</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.746916</td>\n      <td>0.686667</td>\n      <td>0.684552</td>\n      <td>0.682967</td>\n      <td>0.683541</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 05:20:03,772] Trial 14 finished with value: 0.6966666666666667 and parameters: {'learning_rate': 1.7585035670021412e-05, 'batch_size': 4, 'num_train_epochs': 5, 'weight_decay': 0.0003327332013322374, 'gradient_accumulation_steps': 2}. Best is trial 11 with value: 0.72.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 05:17, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.911437</td>\n      <td>0.560000</td>\n      <td>0.634485</td>\n      <td>0.561618</td>\n      <td>0.544012</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.751941</td>\n      <td>0.686667</td>\n      <td>0.692891</td>\n      <td>0.683573</td>\n      <td>0.686223</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.745417</td>\n      <td>0.696667</td>\n      <td>0.707585</td>\n      <td>0.695457</td>\n      <td>0.698607</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.761108</td>\n      <td>0.713333</td>\n      <td>0.714519</td>\n      <td>0.711096</td>\n      <td>0.711683</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 05:25:29,584] Trial 15 finished with value: 0.7133333333333334 and parameters: {'learning_rate': 2.625564726740623e-05, 'batch_size': 4, 'num_train_epochs': 4, 'weight_decay': 0.004972972074366428, 'gradient_accumulation_steps': 3}. Best is trial 11 with value: 0.72.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [114/114 03:21, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.846298</td>\n      <td>0.610000</td>\n      <td>0.594090</td>\n      <td>0.603007</td>\n      <td>0.596956</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.737816</td>\n      <td>0.706667</td>\n      <td>0.702802</td>\n      <td>0.702883</td>\n      <td>0.702830</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.723683</td>\n      <td>0.696667</td>\n      <td>0.690721</td>\n      <td>0.692622</td>\n      <td>0.691448</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:04]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 05:28:58,724] Trial 16 finished with value: 0.7066666666666667 and parameters: {'learning_rate': 3.62767183974613e-05, 'batch_size': 16, 'num_train_epochs': 3, 'weight_decay': 3.094148561059414e-05, 'gradient_accumulation_steps': 1}. Best is trial 11 with value: 0.72.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [375/375 06:42, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.870172</td>\n      <td>0.606667</td>\n      <td>0.574540</td>\n      <td>0.595134</td>\n      <td>0.567055</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.793034</td>\n      <td>0.646667</td>\n      <td>0.631491</td>\n      <td>0.637808</td>\n      <td>0.627862</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.751123</td>\n      <td>0.660000</td>\n      <td>0.660117</td>\n      <td>0.656330</td>\n      <td>0.657357</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.742070</td>\n      <td>0.696667</td>\n      <td>0.695977</td>\n      <td>0.693605</td>\n      <td>0.694515</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.751380</td>\n      <td>0.710000</td>\n      <td>0.715320</td>\n      <td>0.708287</td>\n      <td>0.709965</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 05:35:49,439] Trial 17 finished with value: 0.71 and parameters: {'learning_rate': 1.6980640349864236e-05, 'batch_size': 4, 'num_train_epochs': 5, 'weight_decay': 1.012692234850736e-06, 'gradient_accumulation_steps': 2}. Best is trial 11 with value: 0.72.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [600/600 05:33, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.753883</td>\n      <td>0.670000</td>\n      <td>0.672380</td>\n      <td>0.667088</td>\n      <td>0.669045</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.759259</td>\n      <td>0.713333</td>\n      <td>0.709710</td>\n      <td>0.710233</td>\n      <td>0.708667</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.829193</td>\n      <td>0.696667</td>\n      <td>0.705207</td>\n      <td>0.694714</td>\n      <td>0.697958</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.607900</td>\n      <td>0.953222</td>\n      <td>0.686667</td>\n      <td>0.687343</td>\n      <td>0.684082</td>\n      <td>0.685254</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 05:41:31,123] Trial 18 finished with value: 0.7133333333333334 and parameters: {'learning_rate': 2.5033923850550347e-05, 'batch_size': 4, 'num_train_epochs': 4, 'weight_decay': 0.00033485598494455913, 'gradient_accumulation_steps': 1}. Best is trial 11 with value: 0.72.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [225/225 04:00, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.788092</td>\n      <td>0.666667</td>\n      <td>0.691792</td>\n      <td>0.663634</td>\n      <td>0.665994</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.724573</td>\n      <td>0.696667</td>\n      <td>0.707384</td>\n      <td>0.693720</td>\n      <td>0.697521</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.765732</td>\n      <td>0.703333</td>\n      <td>0.712283</td>\n      <td>0.701686</td>\n      <td>0.704905</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-09 05:45:40,164] Trial 19 finished with value: 0.7033333333333334 and parameters: {'learning_rate': 3.816459268604907e-05, 'batch_size': 4, 'num_train_epochs': 3, 'weight_decay': 0.0025262339383209406, 'gradient_accumulation_steps': 2}. Best is trial 11 with value: 0.72.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Checking for the best set of hyperparameters","metadata":{}},{"cell_type":"code","source":"print(\"Best trial:\")\ntrial = study.best_trial\nprint(f\"  Accuracy: {trial.value}\")\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-09T05:48:04.333692Z","iopub.execute_input":"2024-07-09T05:48:04.334069Z","iopub.status.idle":"2024-07-09T05:48:04.343656Z","shell.execute_reply.started":"2024-07-09T05:48:04.334038Z","shell.execute_reply":"2024-07-09T05:48:04.342598Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Best trial:\n  Accuracy: 0.72\n  Params: \n    learning_rate: 2.0826976349749076e-05\n    batch_size: 4\n    num_train_epochs: 5\n    weight_decay: 1.6916972103647168e-06\n    gradient_accumulation_steps: 1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Training the model with selected best set of hyperparameters","metadata":{}},{"cell_type":"code","source":"best_params = {\n    'learning_rate' : 2.0826976349749076e-05,\n    'batch_size' : 4,\n    'num_train_epochs' : 5,\n    'weight_decay' : 1.6916972103647168e-06,\n    'gradient_accumulation_steps' : 1\n}\nmodel_name = 'bert-base-uncased'\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\ndataset_path = '/kaggle/working/selected.csv'\n# Load the CSV file as a dataset\ndataset = load_dataset('csv', data_files=dataset_path)\n    \n# Since the dataset is loaded with a single key, we access it with 'train'\ndataset = dataset['train']\n\n# Split the dataset into train and test sets\nsplit_dataset = dataset.train_test_split(test_size=0.2)\n    \n# Access the splits\ntrain_dataset = split_dataset['train']\neval_dataset = split_dataset['test']\n\ndef preprocess_function(examples):\n    inputs = tokenizer(examples['text'], truncation=True, padding='max_length')\n    inputs['labels'] = examples['Label']\n    return inputs\n    \ntrain_dataset = train_dataset.map(preprocess_function, batched=True)\neval_dataset = eval_dataset.map(preprocess_function, batched=True)\n    \n# Set format for PyTorch\ntrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\neval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    labels = p.label_ids\n    accuracy = accuracy_score(labels, preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\nargs = TrainingArguments(\n    output_dir='./results',\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=best_params['learning_rate'],\n    per_device_train_batch_size=best_params['batch_size'],\n    per_device_eval_batch_size=best_params['batch_size'],\n    num_train_epochs=best_params['num_train_epochs'],\n    weight_decay=best_params['weight_decay'],\n    save_total_limit=1,\n    metric_for_best_model='accuracy',\n    load_best_model_at_end=True,\n)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-09T06:47:44.021543Z","iopub.execute_input":"2024-07-09T06:47:44.021912Z","iopub.status.idle":"2024-07-09T06:55:26.899984Z","shell.execute_reply.started":"2024-07-09T06:47:44.021886Z","shell.execute_reply":"2024-07-09T06:55:26.898968Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2267860c3b7b4ecaaad24011333cc9b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c865d04b253459b9c096c532edb4251"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9342bbeab2b9427f8c81e5d7e3ddaf9f"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240709_064804-bgfjnj31</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/chethmi-21-University%20of%20Moratuwa/huggingface/runs/bgfjnj31' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/chethmi-21-University%20of%20Moratuwa/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/chethmi-21-University%20of%20Moratuwa/huggingface' target=\"_blank\">https://wandb.ai/chethmi-21-University%20of%20Moratuwa/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/chethmi-21-University%20of%20Moratuwa/huggingface/runs/bgfjnj31' target=\"_blank\">https://wandb.ai/chethmi-21-University%20of%20Moratuwa/huggingface/runs/bgfjnj31</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 07:02, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.750022</td>\n      <td>0.680000</td>\n      <td>0.668299</td>\n      <td>0.665951</td>\n      <td>0.655259</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.670335</td>\n      <td>0.743333</td>\n      <td>0.736191</td>\n      <td>0.739560</td>\n      <td>0.734988</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.770269</td>\n      <td>0.730000</td>\n      <td>0.734226</td>\n      <td>0.731500</td>\n      <td>0.730262</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.614700</td>\n      <td>0.920943</td>\n      <td>0.733333</td>\n      <td>0.739932</td>\n      <td>0.736341</td>\n      <td>0.734422</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.614700</td>\n      <td>1.016352</td>\n      <td>0.730000</td>\n      <td>0.729584</td>\n      <td>0.728029</td>\n      <td>0.728580</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=750, training_loss=0.4631092681884766, metrics={'train_runtime': 458.9921, 'train_samples_per_second': 13.072, 'train_steps_per_second': 1.634, 'total_flos': 1578680506368000.0, 'train_loss': 0.4631092681884766, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"### Fine-tuned model evaluation","metadata":{}},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-07-09T07:04:07.860396Z","iopub.execute_input":"2024-07-09T07:04:07.860804Z","iopub.status.idle":"2024-07-09T07:04:14.446883Z","shell.execute_reply.started":"2024-07-09T07:04:07.860773Z","shell.execute_reply":"2024-07-09T07:04:14.445347Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:06]\n    </div>\n    "},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.670335054397583,\n 'eval_accuracy': 0.7433333333333333,\n 'eval_precision': 0.7361908666877257,\n 'eval_recall': 0.7395597618539339,\n 'eval_f1': 0.7349884325429757,\n 'eval_runtime': 6.571,\n 'eval_samples_per_second': 45.655,\n 'eval_steps_per_second': 5.783,\n 'epoch': 5.0}"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model('/kaggle/working/models/bert-finetuned')","metadata":{"execution":{"iopub.status.busy":"2024-07-09T07:04:39.149759Z","iopub.execute_input":"2024-07-09T07:04:39.150163Z","iopub.status.idle":"2024-07-09T07:04:39.762296Z","shell.execute_reply.started":"2024-07-09T07:04:39.150126Z","shell.execute_reply":"2024-07-09T07:04:39.760931Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Saving the fine-tuned model","metadata":{}},{"cell_type":"code","source":"model_save_path = './my_pretrained_model'\nmodel.save_pretrained(model_save_path)\ntokenizer.save_pretrained(model_save_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T07:14:42.785509Z","iopub.execute_input":"2024-07-09T07:14:42.785892Z","iopub.status.idle":"2024-07-09T07:14:43.425529Z","shell.execute_reply.started":"2024-07-09T07:14:42.785864Z","shell.execute_reply":"2024-07-09T07:14:43.424267Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"('./my_pretrained_model/tokenizer_config.json',\n './my_pretrained_model/special_tokens_map.json',\n './my_pretrained_model/vocab.txt',\n './my_pretrained_model/added_tokens.json',\n './my_pretrained_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"import shutil\nshutil.make_archive('/kaggle/working/my_pretrained_model', 'zip', '/kaggle/working/my_pretrained_model')","metadata":{"execution":{"iopub.status.busy":"2024-07-09T07:30:10.097238Z","iopub.execute_input":"2024-07-09T07:30:10.097653Z","iopub.status.idle":"2024-07-09T07:30:36.168758Z","shell.execute_reply.started":"2024-07-09T07:30:10.097626Z","shell.execute_reply":"2024-07-09T07:30:36.167732Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/my_pretrained_model.zip'"},"metadata":{}}]}]}